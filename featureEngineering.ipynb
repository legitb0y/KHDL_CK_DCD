{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler, RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting(df):\n",
    "    # Khá»Ÿi táº¡o Label Encoder\n",
    "    le = LabelEncoder()\n",
    "    df['author'] = le.fit_transform(df['author'])\n",
    "    df['language'] = le.fit_transform(df['language'])\n",
    "    df['publisher'] = le.fit_transform(df['publisher'])\n",
    "    df['page_format'] = le.fit_transform(df['page_format'])\n",
    "\n",
    "    df.drop(['link', 'title'], axis=1, inplace=True)\n",
    "\n",
    "    df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['genres'] = df['genres'].apply(lambda x: sorted(x))\n",
    "    df['genres'] = df['genres'].apply(lambda x: ','.join(x))\n",
    "    df['genres'] = le.fit_transform(df['genres'])\n",
    "\n",
    "    df['ratings_count'] = df['ratings_count'].apply(lambda x: int(x.replace(',', '')))\n",
    "    df['reviews_count'] = df['reviews_count'].apply(lambda x: int(x.replace(',', '')))\n",
    "    df['num_pages'] = df['num_pages'].astype(float)\n",
    "    df['publish_year'] = df['publish_year'].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillMissingData(data_train, data_val, data_test):\n",
    "    eod_columns = ['publish_year', 'num_pages']\n",
    "    eod_values = [data_train[col].mean() + 3*data_train[col].std() for col in eod_columns]\n",
    "    for col, val in zip(eod_columns, eod_values):\n",
    "        data_train[col].fillna(val, inplace=True)\n",
    "        data_val[col].fillna(val, inplace=True)\n",
    "        data_test[col].fillna(val, inplace=True)\n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistribution(data, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    axes = axes.flatten()\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.hist(data[col], bins=50)\n",
    "        ax.set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "def plotCorrelation(data, columns):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.matshow(data[columns].corr())\n",
    "    plt.xticks(range(len(columns)), columns, rotation=90)\n",
    "    plt.yticks(range(len(columns)), columns)\n",
    "    plt.show()\n",
    "\n",
    "def plotScatter(data, columns, target):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.scatter(data[col], data[target])\n",
    "        ax.set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "def plotBar(data, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.bar(data[col].value_counts().index, data[col].value_counts())\n",
    "        ax.set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "def plotPie(data, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.pie(data[col].value_counts(), labels=data[col].value_counts().index, autopct='%1.1f%%')\n",
    "        ax.set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "def plotHeatmap(data, columns):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.matshow(data[columns].corr())\n",
    "    plt.xticks(range(len(columns)), columns, rotation=90)\n",
    "    plt.yticks(range(len(columns)), columns)\n",
    "    plt.show()\n",
    "\n",
    "def plotHistogram(data, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.hist(data[col], bins=30)\n",
    "        ax.set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "def plotCount(data, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.bar(data[col].value_counts().index, data[col].value_counts())\n",
    "        ax.set_title(col)\n",
    "    plt.show()\n",
    "\n",
    "def plotBox(data, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(20, 5))\n",
    "    for col, ax in zip(columns, axes):\n",
    "        ax.boxplot(data[col])\n",
    "        ax.set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDescription(data, columns):\n",
    "    for col in columns:\n",
    "        print(data[col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewedDists = ['ratings_count', 'reviews_count', 'num_pages', 'language']\n",
    "isUpper_bridge = [True, True, True, True]\n",
    "skewedDists = {col: isUpper_bridge for col, isUpper_bridge in zip(skewedDists, isUpper_bridge)}\n",
    "gaussionDists = ['publish_year']\n",
    "isUpper_boundary = [False]\n",
    "gaussionDists = {col: isUpper_boundary for col, isUpper_boundary in zip(gaussionDists, isUpper_boundary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_handle(data, variable, isShow=False):\n",
    "    upper_boundary=data[variable].mean() + 3 * data[variable].std()\n",
    "    lower_boundary=data[variable].mean() - 3 * data[variable].std()\n",
    "    if isShow:\n",
    "        print(f\"upper_boundary: {upper_boundary}\")\n",
    "        print(f\"lower_boundary: {lower_boundary}\")\n",
    "        print(f\"mean: {data[variable].mean()}\")\n",
    "    return lower_boundary, upper_boundary\n",
    "\n",
    "def outliers_handle_skewed(data, variable, isShow=False):\n",
    "    IQR=data[variable].quantile(0.75) - data[variable].quantile(0.25)\n",
    "    lower_bridge=data[variable].quantile(0.25) - (IQR*3)\n",
    "    upper_bridge=data[variable].quantile(0.75) + (IQR*3)\n",
    "    if isShow:\n",
    "        print(f\"lower_bridge: {lower_bridge}\")\n",
    "        print(f\"upper_bridge: {upper_bridge}\")\n",
    "    return lower_bridge, upper_bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaxScaler = MinMaxScaler()\n",
    "maxAbsScaler = MaxAbsScaler()\n",
    "standardScaler = StandardScaler()\n",
    "robustScaler = RobustScaler()\n",
    "normalizer = Normalizer()\n",
    "quantileTransformer = QuantileTransformer(n_quantiles=640)\n",
    "powerTransformer = PowerTransformer()\n",
    "scalerArr = [minMaxScaler, maxAbsScaler, standardScaler, robustScaler, normalizer, quantileTransformer, powerTransformer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data_train, data_val, data_test, scaler):\n",
    "    scaler.fit(data_train)\n",
    "    data_train = pd.DataFrame(scaler.transform(data_train.copy()), columns=data_train.columns)\n",
    "    data_val = pd.DataFrame(scaler.transform(data_val.copy()), columns=data_val.columns)\n",
    "    data_test = pd.DataFrame(scaler.transform(data_test.copy()), columns=data_test.columns)\n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutlier(data_train, data_val, data_test):\n",
    "    for col in skewedDists:\n",
    "        lower_bridge, upper_bridge = outliers_handle_skewed(data_train, col)\n",
    "        if skewedDists[col]:\n",
    "            data_train.loc[data_train[col]>=upper_bridge,col]=upper_bridge\n",
    "            data_val.loc[data_val[col]>=upper_bridge,col]=upper_bridge\n",
    "            data_test.loc[data_test[col]>=upper_bridge,col]=upper_bridge\n",
    "        else:\n",
    "            data_train.loc[data_train[col]<=lower_bridge,col]=lower_bridge\n",
    "            data_val.loc[data_val[col]<=lower_bridge,col]=lower_bridge\n",
    "            data_test.loc[data_test[col]<=lower_bridge,col]=lower_bridge\n",
    "        \n",
    "    for col in gaussionDists:\n",
    "        lower_boundary, upper_boundary = outliers_handle(data_train, col)\n",
    "        if gaussionDists[col]:\n",
    "            data_train.loc[data_train[col]>=upper_boundary,col]=upper_boundary\n",
    "            data_val.loc[data_val[col]>=upper_boundary,col]=upper_boundary\n",
    "            data_test.loc[data_test[col]>=upper_boundary,col]=upper_boundary\n",
    "        else:\n",
    "            data_train.loc[data_train[col]<=lower_boundary,col]=lower_boundary\n",
    "            data_val.loc[data_val[col]<=lower_boundary,col]=lower_boundary\n",
    "            data_test.loc[data_test[col]<=lower_boundary,col]=lower_boundary\n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def featureSelection(X, y): \n",
    "    X_kbest = SelectKBest(f_classif, k = 5).fit(X, y)\n",
    "    ix = X_kbest.get_support() \n",
    "    data = pd.DataFrame(X_kbest.transform(X), columns = X.columns.values[ix])\n",
    "    # print(data.columns.values)\n",
    "    return data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(X_train, X_val, X_test):\n",
    "    X_train, X_val, X_test = fillMissingData(X_train, X_val, X_test)\n",
    "    X_train, X_val, X_test = handleOutlier(X_train, X_val, X_test)\n",
    "    X_train, X_val, X_test = scaleData(X_train, X_val, X_test, powerTransformer)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeatures(X_train, y_train, X_val, X_test):\n",
    "    featureArray = featureSelection(X_train, y_train)\n",
    "    X_train = X_train[featureArray]\n",
    "    X_val = X_val[featureArray]\n",
    "    X_test = X_test[featureArray]\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessing(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # check for doublications\n",
    "    print(f\"Number of duplicated rows: {df.duplicated().any().sum()}\")\n",
    "    if df.duplicated().any().sum() > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # check for missing values\n",
    "    print(f\"Number of missing values: \\n{df.isnull().sum()}\")\n",
    "\n",
    "    df = extracting(df)\n",
    "\n",
    "    data = df.copy()\n",
    "    y = data['avg_ratings']\n",
    "    X = data.drop(['avg_ratings'], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, X_test = preprocessor(X_train, X_val, X_test)\n",
    "    X_train, X_val, X_test = selectFeatures(X_train, y_train, X_val, X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 1\n",
      "Number of missing values: \n",
      "title              0\n",
      "author             0\n",
      "language         211\n",
      "avg_ratings        0\n",
      "ratings_count      0\n",
      "reviews_count      0\n",
      "publisher        372\n",
      "publish_year      61\n",
      "num_pages        124\n",
      "page_format       44\n",
      "genres             0\n",
      "link               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>reviews_count</th>\n",
       "      <th>publish_year</th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098921</td>\n",
       "      <td>0.403950</td>\n",
       "      <td>0.692743</td>\n",
       "      <td>8.326673e-17</td>\n",
       "      <td>-0.071480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098921</td>\n",
       "      <td>0.138715</td>\n",
       "      <td>0.027054</td>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>-0.060487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.098921</td>\n",
       "      <td>0.361205</td>\n",
       "      <td>0.242546</td>\n",
       "      <td>-8.326673e-17</td>\n",
       "      <td>-1.323114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098921</td>\n",
       "      <td>-0.574367</td>\n",
       "      <td>-0.957856</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.015445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098921</td>\n",
       "      <td>0.698340</td>\n",
       "      <td>1.499411</td>\n",
       "      <td>1.387779e-16</td>\n",
       "      <td>0.052767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  ratings_count  reviews_count  publish_year  num_pages\n",
       "0  0.098921       0.403950       0.692743  8.326673e-17  -0.071480\n",
       "1  0.098921       0.138715       0.027054 -2.775558e-17  -0.060487\n",
       "2  0.098921       0.361205       0.242546 -8.326673e-17  -1.323114\n",
       "3  0.098921      -0.574367      -0.957856  0.000000e+00   0.015445\n",
       "4  0.098921       0.698340       1.499411  1.387779e-16   0.052767"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = proccessing('data_10000.csv')\n",
    "X_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
